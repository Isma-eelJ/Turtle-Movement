---
title: "Bobb_current_analysis"
author: "Isma-eel Jattiem"
format:
  html:
    embed-resources: true
---

```{r}
# Load Library
library(ncdf4)
library(lubridate)
library(dplyr)
library(progressr)
library(foreach)
library(doParallel)
library(ggplot2)
library(viridis)       
library(sf)            
library(rnaturalearth) 
library(rnaturalearthdata)

```


```{r}
# Setting up environment
sf_use_s2(FALSE)

# Enable progress reporting
# handlers(global = TRUE)
# handlers("progress")

```

## Loading in Data
```{r}
# Load in Data
currents <- nc_open("current_Bob.nc")

```

## Processing NetCDF file

### Extracting Dimensions
```{r}
# Extract Dimensions
dim_lon <- ncvar_get(currents, "longitude")
dim_lat <- ncvar_get(currents, "latitude")
dim_depth <- ncvar_get(currents, "depth")
dim_time <- ncvar_get(currents, "time")

```


### Calculating Grid
```{r}
# Calculate grid sizes for better memory estimation
n_lon <- length(dim_lon)
n_lat <- length(dim_lat)
n_depth <- length(dim_depth)
n_time <- length(dim_time)
total_points <- n_lon * n_lat * n_depth * n_time
cat("Grid size:", n_lon, "x", n_lat, "x", n_depth, "x", n_time, 
    "=", total_points, "points\n")

```

### Extracting Variables
```{r echo=FALSE}
# Extract Variables 
uo <- ncvar_get(currents, "uo", collapse_degen=FALSE)
vo <- ncvar_get(currents, "vo", collapse_degen=FALSE)

```

### Converting time to SI units
```{r}
# Time Conversion
t_units <- ncatt_get(currents, "time", "units")
t_ustr <- t_units$value

# Print the original time units string to better understand its format
cat("Time units string:", t_ustr, "\n")

# More robust time parsing - this handles common NetCDF time formats
if (grepl("since", t_ustr, ignore.case = TRUE)) {
  # Extract the reference date part after "since"
  date_part <- sub(".*since\\s+([^\\s]+).*", "\\1", t_ustr)
  
  # Try to parse the reference date
  ref_date <- try(as.Date(date_part), silent = TRUE)
  
  if (inherits(ref_date, "try-error") || is.na(ref_date)) {
    # If standard parsing fails, try various common formats
    formats <- c("%Y-%m-%d", "%Y/%m/%d", "%d-%m-%Y", "%d/%m/%Y")
    for (fmt in formats) {
      ref_date <- try(as.Date(date_part, format = fmt), silent = TRUE)
      if (!inherits(ref_date, "try-error") && !is.na(ref_date)) {
        break
      }
    }
  }
  
  # If we still couldn't parse the date, print a warning
  if (inherits(ref_date, "try-error") || is.na(ref_date)) {
    warning("Could not parse reference date: ", date_part)
    # Default to a placeholder date
    ref_date <- as.Date("1970-01-01")
  }
  
  # Determine the time unit (days, hours, seconds, etc.)
  time_unit <- "seconds" # default
  if (grepl("day", t_ustr, ignore.case = TRUE)) {
    time_unit <- "days"
  } else if (grepl("hour", t_ustr, ignore.case = TRUE)) {
    time_unit <- "hours"
  } else if (grepl("minute", t_ustr, ignore.case = TRUE)) {
    time_unit <- "minutes"
  } else if (grepl("second", t_ustr, ignore.case = TRUE)) {
    time_unit <- "seconds"
  }
  
  # Convert time values based on the unit
  cat("Using reference date:", as.character(ref_date), "with unit:", time_unit, "\n")
  date <- ref_date
  if (time_unit == "days") {
    date <- ref_date + dim_time
  } else if (time_unit == "hours") {
    date <- ref_date + dhours(dim_time)
  } else if (time_unit == "minutes") {
    date <- ref_date + dminutes(dim_time)
  } else {
    date <- ref_date + dseconds(dim_time)
  }
} else {
  warning("Time units string does not contain 'since'. Using time values as is.")
  date <- dim_time
}

```

### Creating data frame from the NetCDF file

#### Parallel Processing

Setting up the environment to run processes in parallel to shorten run time and 
make it more efficient 
```{r}
# Set up parallel processing
n_cores <- min(6, parallel::detectCores() - 1)
registerDoParallel(cores = n_cores)
cat("Using", n_cores, "cores for parallel processing\n")
```

#### Running Data Extraction
Creating loop to extract data and create data frame

Loop is run per depth so the data will be extracted in groups and they are grouped via depth

```{r}
# Function to process data in chunks
process_chunks <- function() {
  # Process by depth levels to save memory
  with_progress({
    p <- progressor(steps = n_depth)
    
    # Explicitly capture all needed variables for parallel processing
    dim_lon_p <- dim_lon
    dim_lat_p <- dim_lat
    dim_depth_p <- dim_depth
    date_p <- date
    uo_p <- uo
    vo_p <- vo
    n_lon_p <- n_lon
    n_lat_p <- n_lat
    n_time_p <- n_time
    n_depth_p <- n_depth
    
    result_list <- foreach(d_idx = 1:n_depth_p, .combine = rbind, 
                           .packages = c("dplyr", "lubridate")) %dopar% {
                             # Update progress
                             p(message = sprintf("Processing depth level %d of %d", d_idx, n_depth_p))
                             
                             # Pre-allocate a data frame for this depth level
                             depth_df <- data.frame(
                               lon = rep(dim_lon_p, each = n_lat_p * n_time_p),
                               lat = rep(rep(dim_lat_p, each = n_time_p), n_lon_p),
                               depth = dim_depth_p[d_idx],
                               time = rep(date_p, n_lon_p * n_lat_p),
                               uo = as.vector(uo_p[, , d_idx, ]),
                               vo = as.vector(vo_p[, , d_idx, ])
                             )
                             
                             return(depth_df)
                           }
    
    return(result_list)
  })
}

```

#### Creating Data Frame
```{r echo=FALSE}
# Run the chunked processing
current_df <- process_chunks()
```

## Saving Files

There are two different methods for saving the data extracted from the NetCDF file so that the data is useful for analysis

### Option 1

Saving as an .rds file
```{r echo=FALSE}
saveRDS(current_df, "processed_currents.rds")

```

### Option 2

Saving as the standard .csv file
```{r echo=FALSE}
write.csv(current_df, "processed_currents.csv", row.names = FALSE)

```

Unlikely to use Option 2 because it takes way too long to run. Option 1 is way more time and space effecient

## Analysing Data

This section will be dedicated to doing various analysis on the data now that is in a workable format. 

I'll start now with a simple analysis of current speed and direction

### Current Speed and Current Direction

```{r}
# Analyse Data
current_analysis <- current_df %>%
  filter(depth == min(depth)) %>% 
  group_by(time) %>% 
  mutate(
    current_speed = sqrt(uo^2 + vo^2),
    current_direction = (90 - (atan2(vo, uo) * 180/pi)) %% 360)

```

## Plotting the Data

Still working on creating an animation
(Very nervous to run due to the size of the file)

### Basic plot
A plot of Ocean currents on a singular day (2023-01-26)
 - This is just to practice and learn how to work with the data
 - It also uses a subset of the data allowing for quick analysis

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")

# Setting Bounding Box
min_lon <- min(current_analysis$lon, na.rm = TRUE)
max_lon <- max(current_analysis$lon, na.rm = TRUE)
min_lat <- min(current_analysis$lat, na.rm = TRUE)
max_lat <- max(current_analysis$lat, na.rm = TRUE)

# Filtering time
current_analysis_1 <- current_analysis %>% 
  filter(time == "2023-01-26") %>% 
  na.omit()

# removing some data points to make the diagram more clear
skip <- 10
current_subset <- current_analysis_1[seq(1, nrow(current_analysis_1), by = skip), ]

# Creating the plot using ggplot2
ggplot() +
  geom_segment(
    data = current_subset,
    aes(x = lon, y = lat, xend = lon + uo, yend = lat + vo, color = current_speed),
    # Adjust arrow size by modifying the length (e.g., unit(0.15, "cm") to another value)
    arrow = arrow(length = unit(0.15, "cm"), type = "closed"),
    # Adjust arrow thickness by changing 'linewidth'
    linewidth = 0.5) +
  scale_color_viridis(option = "inferno") +
  geom_sf(data = world, fill = "grey80", color = "black", linewidth = 0.5) +
  coord_sf(xlim = c(min_lon, max_lon), ylim = c(min_lat, max_lat), expand = FALSE) +
  theme_minimal() +
  labs(title = "Ocean Current Vectors For Bobb (2023-01-26)",  # Modify the title as needed
       x = "Longitude",
       y = "Latitude",
       color = "Speed (m/s)")

```

